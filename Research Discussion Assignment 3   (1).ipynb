{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Christina Valore\n",
    "Data 612\n",
    "\n",
    "## YouTube recommender bias\n",
    "\n",
    "\n",
    "### Question:\n",
    "\n",
    "As more systems and sectors are driven by predictive analytics, there is increasing awareness of the possibility and pitfalls of algorithmic discrimination. In what ways do you think Recommender Systems reinforce human bias? Reflecting on the techniques we have covered, do you think recommender systems reinforce or help to prevent unethical targeting or customer segmentation?  Please provide one or more examples to support your arguments.\n",
    "\n",
    "### Response:\n",
    "\n",
    "With any type of machine learning, there is always the chance that the machine can perform in negative ways. Recommender systems absolutely reinforce human bias as they are learning from human input, which is inherently biased. \n",
    "\n",
    "One example of this enforced bias is the Youtube recommender system. The YouTube recommender algorithm takes into account what the user previously watched, time spent watching a video and the number of views. Using these metrics along with consideration of how often this video is viewed, YouTube will start recommending this type of content to other similar users. \n",
    "\n",
    "This caused an issue with the rise in conspiracy videos and fake news stories, particulary for tragic events like the massacre that occurred in Las Vegas. Conspiracy videos claiming the shooting was a hoax had ammassed millions of views. Following the YouTube algorithm, videos like this became top trending even though they were not fact and only conspiricies. In addition, videos like these fueled the creation of other hateful, racist content. \n",
    "\n",
    "To combat this, YouTube created software to stop conspiracy videos from going viral in the wake of a a tragic event. YouTube also hired human evaluators to review specific content within a set of guidelines. YouTube then took the feedback and used it to train the recommender system. \n",
    "\n",
    "This example highlights how an algorithm that goes uncheck with no failsafes, can cause unintended consequences. YouTube was proactive in continuing to ensure this type of content was flagged and not reccommended based on the original algorithm. It is important to note that even though conspiracy videos are not recommended using their algorithm, they are not removed from the platform as YouTube supports free speech. \n",
    "\n",
    "\n",
    "Source: https://www.washingtonpost.com/technology/2019/01/25/youtube-is-changing-its-algorithms-stop-recommending-conspiracies/?utm_term=.806b59457da3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
